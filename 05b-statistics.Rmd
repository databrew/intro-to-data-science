# Significance statistics

## Basics 


## Variable types & statistics

## Comparison tests

These distributions are different, but there IS a lot of overlap. We need a statistical test to check whether the difference between these two distributions is statistically significant.

This is a t-test!

We use t-tests to test for differences in means when out independent variable is CATEGORICAL and our dependent variable is CONTINUOUS. In this case:

* Independent variable / treatments: baseline VS pandemic (categorical)
* Dependent variable / outcome variable: frequency of nature access (borderline continuous, can be treated categorically)

```{r,eval=FALSE}
t.test(covdat$covfreqnum, covdat$basefreqnum)
```

Interpret together:

* Alternative hypothesis
* p value 
* Difference in means
* Confidence interval
* T: the difference between groups represented in units of standard error. 

We use an ANOVA test in cases where there are more than two groups we want to compare.

* Independent variable : Effect of the pandemic on employment status (increase, decrease, no change)
* Dependent variable: Anxiety scores

```{r,eval=FALSE}
myANOVA<-aov(covdat$GAD7~covdat$emp)
summary(myANOVA)
```

Our p-value is low, which means there are differences between groups. But the ANOVA doesn't actually tell us which two-way differences are significant. For that we need a follow up test: a Tukey's HSD test.

```{r,eval=FALSE}
TukeyHSD(myANOVA)
```

People who have experienced a change in employment have higher anxiety scores, regardless of whether they have more or fewer working hours. 

We usually use a bar graph to depict the relationship between a categorical independent variable and a continuous dependent variable (the cases where we typically run T-tests and Anovas). 

```{r,eval=FALSE}
n <-table(covdat$emp); n
means<-aggregate(GAD7~emp, data=covdat, FUN=mean); means
sds<-aggregate(GAD7~emp, data=covdat, FUN=sd); sds
sds$se<-sds$GAD7/sqrt(n); sds

# basic barplot 
bar<-barplot(height=means$GAD7, names.arg=means$emp,
             xlab="Change in working hours", ylab="GAD7 scores", ylim=c(0,10))
# error bars
segments(bar, means$GAD7 - sds$se * 2, bar,
         means$GAD7 + sds$se * 2, lwd = 1.5)

```

If both our independent and dependent variables are BOTH CATEGORICAL, we use a different test called a Chi-squared test.

* Independent variable / treatments : race (catgorical)
* Dependent variable / outcome variable : Self reported change in nature (gain, loss, no change) (categorical)

```{r,eval=FALSE}
table(covdat$natchan) # most people experienced nature losses during the pandemic

# We can create a CONTINGENCY TABLE to break down these totals by a second variable (race):
table(covdat$race, covdat$natchan)

# Are these differences in the proportion of people loosing/gaining 
# nature statistically significant?
CS<-chisq.test(table(covdat$race, covdat$natchan)) # the contingency table is your input to the chisq.test() function
CS$p.value 
CS$residuals 
```

How to interpret: 

* Because the p-value of our test is less the 0.05, we conclude that there is a relationship between race and change in nature exposure during the pandemic.
* positive residual = positive association: being non white is associated with having less nature
* negative residual = negative association: being non-white lowers changes of haveing more or the same nature.

Other helpful outputs of the chi2 test include: 

* CS$expected # If there was no relationship, what counts would we expect?
* CS$observed # our actual counts


## Correlation tests


If we want to visualize a relationship between two continuous variables, we typically plot them with a scatter-plot, and we test for a statistical relationship using linear regression. These gives us a visual and test for an association/correlation between the two variables, but can not establish that one variable is causing a change in the other. 

For example: 



```{r,eval=FALSE}
# there are lots of overlapping points. Let's use a rgb scale 
# to make dots partially transparent so that the pattern easier to see:
plot(covdat$PHQ8~covdat$GAD7, pch=16, 
     col=rgb(red = 0, green = .5, blue = 1, alpha = 0.25), 
     xlab="Anxiety (GAD7)", ylab="Depression (PHQ8)")

# We can use a linear regression model to try to measure the relationship 
# between two continuous variables:
mod<-lm(covdat$PHQ8~covdat$GAD7)
summary(mod)

# We can add a line to our plot representing the linear regression
abline(mod, col="red2", lwd=3)
```

Let's interpret the outcome of the linear regression:

* coefficient = 0.9 : the slope of the line. For every 1 point increase in anxiety, we would expect a 0.9 point increase in depression.
* r2 = .68 : 68% of the variation in depression is captured by/explained by variation in anxiety 
* p-value < 2.2e-16 : This is the probability that there is no relationship (a slope of zero) between anxiety and depression in reality given the pattern we see in our sample. Less than 0.05 - a statistically significant relationship. 

We would be remiss to conclude based on this graph that being anxious makes people depressed. Instead, we conclude that people who have higher anxiety scores also tend to have higher depression scores. In other words, depression and anxiety are POSITIVELY CORRELATED. 



Similarly if we plotted nature exposure on the x-axis, and found a negative correlation, we could not really conclude that spending little time in nature was causing people to be depressed, or the spending lots of time in nature was alleviating depression. CORRELATION DOES NOT EQUAL CAUSATION. 

Lots of things might explain a persons level of depression, mental health history and having lost their job for example. With an observational data-set, sometimes the best that we can do is to try and carefully think of all of the other variables at play, and account for them, and to test specific hypotheses where a causal relationship is backed up by theory and previous research rather than searching our data for any patterns that emerge (this is called data-mining).  

```{r,eval=FALSE}
SimpleReg<-lm(covdat$PHQ8~covdat$covfreqnum)
summary(SimpleReg)

depmod<-lm(data=covdat, PHQ8 ~ covfreqnum + cov + famcov + emp + 
             pc + mhh + week + gender + kids)
summary(depmod)
```

Interpreting Regression Summaries: 

* Coefficient: Adding other predictors into our model reduces the effect size of the nature frequency variable because some of the variation in depression that was originally captured by variation in nature frequency is now attributed to other things that also explain differences in depression scores across our sample.
* Holding all the other factors in our model constant, a shift up 1 point in nature frequencey score is associated with .27 decrease in mean depression scores.
* r2: On it's own, nature frequency explains very little of the variation in depression. All of our predictors together explain 22 percent of the variation in Depression scores
* p-value: Because we have a p-value <0.05, our data supports the hypothesis that spending time in nature more frequently is associated with lower depression. 










