# Working with text {#text}

#### Learning goals {-}

* Learn to apply the most common `R` tools for working with text.


## Basics {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
library(gsheet)
survey <- gsheet2tbl('https://docs.google.com/spreadsheets/d/1iVt9FX9J2iv3QFKBM7Gzb9dgva70XrW1lxMV4hpekeo/edit?resourcekey#gid=204634767')
names(survey) <- c('time', 'sex', 'age','sib', 'dad_mus', 'person_mus', 'joe_mus_is', 'eyesight', 'height', 'shoe_size', 'bday', 'money_or_love', 'rps_skill', 'num_pan', 'cats_dogs', 'first_name', 'last_name')

```

#### paste0 {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}

# combine first name and last name 
paste0(survey$first_name,' ', survey$last_name)
paste0(survey$first_name, ' ', survey$sex)

# Make a sentence
paste0('First name is ', survey$first_name,' and last name is  ', survey$last_name)

# combine first and last name to make new variable called full_name
survey <- survey %>% mutate(full_name = paste0(survey$first_name, '_', survey$last_name ))


```

#### tolower & toupper {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}

# combine first name and last name 
tolower(survey$money_or_love)
toupper(survey$money_or_love)

# overwrite money_or_love
survey <- survey %>% mutate(money_or_love = tolower(money_or_love))

```

#### nchar {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
# count how many characters are in each observations in sex
nchar(survey$eyesight)

# create a boolean vector of the number of observations with 32 characters
nchar(survey$eyesight) ==42

# get number of observations with character 42
length(which(nchar(survey$eyesight) ==42))
sum(nchar(survey$eyesight) == 42)

```

##### substr {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
# grab only the first letter fo sex
substr(survey$sex, start = 1, stop = 1)

# remove the "s" from cats and dogs
substr(survey$cats_dogs, start = 1, stop = 3)
```

#### gsub {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}

# replace "Deeply captivating" with "captivating
gsub('Deeply captivating', 'captivating', survey$joe_mus_is)

# remove long strings
gsub('(glasses or contacts, but I can get by without them)','', survey$eyesight, fixed = TRUE)

# overwrite variable
survey <- survey %>% mutate(eyesight = gsub('(glasses or contacts, but I can get by without them)','', eyesight, fixed = TRUE))

```

#### grepl {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
# how many times does Brew show up in full_name
grepl('Brew', survey$full_name)

# Born in may
grepl('05', survey$bday)

# Born in 2000
grepl('2000', survey$bday)

# filter data by those born in 2000
survey %>% filter(grepl('2000', bday))

```


#### Exercise {-} 

1) Create a new variable called first_name_sex that combines the first name and the sex of each individual

2) Create a new variable called backwards_name that combines last name and first name 

3) Make the backwards_name variable capitalized

4) Replace "both" in the money_or_love variable with "Money & Love"

5) Get only the first character dad_mus variable 

6) How many total characters are in the column eyesight

7) How many characters did Joe write for the eyesight question

8) How many people in the data were born on the 4th day of the month

9) Create a new variables called month_born that has only the month of from the bday variable 

10) Do the same thing for year 

11) Filter the data set by those born and 2001 and prefer being money


#### Team exercises  {-}

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}

library(dplyr)
library(readr)
shake <- read_csv('https://raw.githubusercontent.com/databrew/intro-to-data-science/main/data/Shakespeare_data.csv')

```

All good? Great. Let's go.

1. How may rows in the data?

2. Create a dataframe named `spoken`. This should be those lines which are spoken by an actor/actress (figure it out).

3. How many lines are spoken?

4. Create a column called `first_word`. This should be the first word of each spoken line.

5. What is the most common first word spoken?

6. Create a boolean column named "King". This should indicate whether or not the word "King" was spoken in any given line.

7. Improve the above by making sure that it includes both lower and uppercase variations of "king".

8. Figure out which play has the word "king" mentioned most?

9. What percentage of lines in Hamlet mention the word "king?

10. How many times does the word "woman" appear in each play?

11. How many words are there in all Shakespeare plays?

12. How many letters are there in each Shakespeare play?

13. Which character says the most words?

14. Which character says the least words?

15. What is the lines(s) of the character who says the least words?

16. Make a table of plays with one row per play and variables being: (a) number of lines, (b) number of words, (c) number of characters, (d) number of letters, (e) number of mentions of "Brew".



#### Exercises (Trump tweets)  {-}

Let's run the below to get started.

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}

library(dplyr)
library(readr)
library(tidytext)
trump <- read_csv('https://raw.githubusercontent.com/databrew/intro-to-data-science/main/data/trumptweets.csv')

```

1. In the current format, one row of data is equal to one <what>?

2. Create a variable called `line`. This should be 1, 2, 3, 4, etc.

3. Create a variable called `text`. This should be an exact copy of `content`.

4. Use the `unnest_tokens` function to reshape the data for better text processing.

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
simple <- trump %>%
  select(-mentions, -hashtags, -geo, -content, -link, -id) %>%
  unnest_tokens(word, text)
```

5. What format is the data in now (ie, one row is equal to <what>)?

6. Take a minute to read about the `tidytext` package at https://www.tidytextmining.com/tidytext.html.

7. What is the most common word used by Trump?

8. Use `substr` to create a `year` variable.

9. What is the most common word used by Trump each year?

10. Create a variable named `month` using `substr`.

11. What is the most common word used by Trump each month?

12. Create a dataframe with one word per row, and a column called `freq` saying how many times that word was used.

13. Load up the `wordcloud` library.

14. Subset the dataframe created in number 12 to only include the top 100 words.

15. Create a wordcloud of Trump's top 100 words.

16. Are you ready to do some sentiment analysis? Great.

17. Create a dataframe named `sentiments` by running the following: `sentiments <- read_csv('https://raw.githubusercontent.com/databrew/intro-to-data-science/main/data/sentiments.csv')`

```{r, eval = FALSE, echo = FALSE}
sentiments <- get_sentiments("nrc")
uns <- sort(unique(sentiments$sentiment))
sentiments <- sentiments %>%
  group_by(word) %>%
  summarise(anger = length(which(sentiment == 'anger')),
            anticipation = length(which(sentiment == 'anticipation')),
            disgust = length(which(sentiment == 'disgust')),
            fear = length(which(sentiment == 'fear')),
            joy = length(which(sentiment == 'joy')),
            negative = length(which(sentiment == 'negative')),
            positive = length(which(sentiment == 'positive')),
            sadness = length(which(sentiment == 'sadness')),
            surprise = length(which(sentiment == 'surprise')),
            trust = length(which(sentiment == 'trust')))
write_csv(out, 'data/sentiments.csv')
```

18. What is the `sentiments` dataset?

19. Create another dataset named `polarity` by running the following: `polarity <- get_sentiments("afinn")`

20. Use `left_join` to combine polarity and sentiments into one dataset named `emotions`.


```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
emotions <- left_join(sentiments, polarity) %>% filter(!duplicated(word))
```

21. Use `left_join` to combine the `trump` data and the `emotions` data.

```{r,echo=TRUE,collapse=TRUE, eval = FALSE}
simple <- left_join(simple, emotions)
```

22. Have a look at the `simple` (Trump) data. What do you see?

23. Get an overall polarity score (using the `value` variable) for the entire dataset. Is it positive or negative?

24. How many words were emotionally associated with "anger" in 2015?

25. What percentage of words were associated with "fear" by year?

26. What is the average sentiment polarity by year?

27. What is Trump's most positive tweet?

28. What month was Trump's most negative month?

29. What percentage of Trump tweets have more sadness than joy by year/month?
